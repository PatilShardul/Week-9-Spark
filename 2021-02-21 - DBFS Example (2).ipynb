{"cells":[{"cell_type":"markdown","source":["## Overview\n","\n","This notebook will show you how to create and query a table or DataFrame that you uploaded to DBFS. [DBFS](https://docs.databricks.com/user-guide/dbfs-databricks-file-system.html) is a Databricks File System that allows you to store data for querying inside of Databricks. This notebook assumes that you have a file already inside of DBFS that you would like to read from.\n","\n","This notebook is written in **Python** so the default cell type is Python. However, you can use different languages by using the `%LANGUAGE` syntax. Python, Scala, SQL, and R are all supported."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"96816ed7-b08a-4ca3-abb9-f99880c3535d"}}},{"cell_type":"code","source":["from pyspark.sql.functions import *"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7b8fe474-03de-44ad-89b5-fac5059c5231"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["# Create a spark dataframe in the "],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"62edcf2c-1e60-4c5a-8b31-e074cd53759a"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# File location and type\n","file_location = \"/FileStore/tables/computerlogs/CpuLogData2019*.csv\"\n","file_type = \"csv\"\n","\n","# CSV options\n","infer_schema = \"true\"\n","first_row_is_header = \"true\"\n","delimiter = \",\"\n","\n","# The applied options are for CSV files. For other file types, these will be ignored.\n","df = spark.read.format(file_type) \\\n","  .option(\"inferSchema\", infer_schema) \\\n","  .option(\"header\", first_row_is_header) \\\n","  .option(\"sep\", delimiter) \\\n","  .load(file_location)\n","\n","df = df.select(\"DateTime\",\"keyboard\",\"mouse\",\"user_name\")\\\n","        .withColumn('Date', split(df['DateTime'], ' ').getItem(0))\\\n","        .withColumn('Time', split(df['DateTime'], ' ').getItem(1))\\\n","        .withColumn('DateTime', to_timestamp(df['DateTime']))\n","print(df.count())"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6482be4c-f067-47c9-b0ac-35c938b94601"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">4122\n</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">4122\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["storage_account_name = \"saweekeightsparkstorage\"\n","\n","# Azure Storage Account Key\n","storage_account_key = \"Your Storage Account key that you will find in access keys in your Storage Account \"\n","\n","# Azure Storage Account Source Container\n","container = \"Your container name\"\n","\n","# Set the configuration details to read/write\n","spark.conf.set(\"fs.azure.account.key.{0}.blob.core.windows.net\".format(storage_account_name), storage_account_key)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"60e20bfc-3698-4474-a424-4cd12edb8abc"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["#Mount Azure Storage Account in your Databricks \n","dbutils.fs.mount(\n","   source = \"wasbs://{0}@{1}.blob.core.windows.net\".format(container, storage_account_name),\n","   mount_point = \"/mnt/sparklogfile\",\n","   extra_configs = {\"fs.azure.account.key.{0}.blob.core.windows.net\".format(storage_account_name): storage_account_key}\n","  )"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"fe2eea55-3cdb-42ba-a18e-99ca27569eba"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"<div class=\"ansiout\"><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">ExecutionError</span>                            Traceback (most recent call last)\n<span class=\"ansi-green-fg\">&lt;command-3348328071542889&gt;</span> in <span class=\"ansi-cyan-fg\">&lt;module&gt;</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      2</span>    source <span class=\"ansi-blue-fg\">=</span> <span class=\"ansi-blue-fg\">&#34;wasbs://{0}@{1}.blob.core.windows.net&#34;</span><span class=\"ansi-blue-fg\">.</span>format<span class=\"ansi-blue-fg\">(</span>container<span class=\"ansi-blue-fg\">,</span> storage_account_name<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">,</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      3</span>    mount_point <span class=\"ansi-blue-fg\">=</span> <span class=\"ansi-blue-fg\">&#34;/mnt/sparklogfile&#34;</span><span class=\"ansi-blue-fg\">,</span>\n<span class=\"ansi-green-fg\">----&gt; 4</span><span class=\"ansi-red-fg\">    </span>extra_configs <span class=\"ansi-blue-fg\">=</span> <span class=\"ansi-blue-fg\">{</span><span class=\"ansi-blue-fg\">&#34;fs.azure.account.key.{0}.blob.core.windows.net&#34;</span><span class=\"ansi-blue-fg\">.</span>format<span class=\"ansi-blue-fg\">(</span>storage_account_name<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span> storage_account_key<span class=\"ansi-blue-fg\">}</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      5</span>   )\n\n<span class=\"ansi-green-fg\">/local_disk0/tmp/1614143901425-0/dbutils.py</span> in <span class=\"ansi-cyan-fg\">f_with_exception_handling</span><span class=\"ansi-blue-fg\">(*args, **kwargs)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    322</span>                     exc<span class=\"ansi-blue-fg\">.</span>__context__ <span class=\"ansi-blue-fg\">=</span> <span class=\"ansi-green-fg\">None</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    323</span>                     exc<span class=\"ansi-blue-fg\">.</span>__cause__ <span class=\"ansi-blue-fg\">=</span> <span class=\"ansi-green-fg\">None</span>\n<span class=\"ansi-green-fg\">--&gt; 324</span><span class=\"ansi-red-fg\">                     </span><span class=\"ansi-green-fg\">raise</span> exc\n<span class=\"ansi-green-intense-fg ansi-bold\">    325</span>             <span class=\"ansi-green-fg\">return</span> f_with_exception_handling\n<span class=\"ansi-green-intense-fg ansi-bold\">    326</span> \n\n<span class=\"ansi-red-fg\">ExecutionError</span>: An error occurred while calling o257.mount.\n: java.rmi.RemoteException: java.lang.IllegalArgumentException: requirement failed: Directory already mounted: /mnt/sparklogfile; nested exception is: \n\tjava.lang.IllegalArgumentException: requirement failed: Directory already mounted: /mnt/sparklogfile\n\tat com.databricks.backend.daemon.data.client.DbfsClient.send0(DbfsClient.scala:128)\n\tat com.databricks.backend.daemon.data.client.DbfsClient.sendIdempotent(DbfsClient.scala:68)\n\tat com.databricks.backend.daemon.dbutils.DBUtilsCore.mount(DBUtilsCore.scala:470)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n\tat py4j.Gateway.invoke(Gateway.java:295)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:251)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.lang.IllegalArgumentException: requirement failed: Directory already mounted: /mnt/sparklogfile\n\tat scala.Predef$.require(Predef.scala:281)\n\tat com.databricks.backend.daemon.data.server.DefaultMetadataManager.$anonfun$insertMount$1(MetadataManager.scala:404)\n\tat com.databricks.backend.daemon.data.server.DefaultMetadataManager.$anonfun$modifyAndVerify$1(MetadataManager.scala:670)\n\tat com.databricks.backend.daemon.data.server.DefaultMetadataManager.withRetries(MetadataManager.scala:504)\n\tat com.databricks.backend.daemon.data.server.DefaultMetadataManager.modifyAndVerify(MetadataManager.scala:659)\n\tat com.databricks.backend.daemon.data.server.DefaultMetadataManager.insertMount(MetadataManager.scala:412)\n\tat com.databricks.backend.daemon.data.server.handler.MountHandler.receive(MountHandler.scala:89)\n\tat com.databricks.backend.daemon.data.server.handler.CEMountHandler.receive(MountHandler.scala:141)\n\tat com.databricks.backend.daemon.data.server.session.SessionContext.$anonfun$queryHandlers$1(SessionContext.scala:97)\n\tat com.databricks.backend.daemon.data.server.session.SessionContext.$anonfun$queryHandlers$1$adapted(SessionContext.scala:96)\n\tat scala.collection.immutable.List.foreach(List.scala:392)\n\tat com.databricks.backend.daemon.data.server.session.SessionContext.queryHandlers(SessionContext.scala:96)\n\tat com.databricks.backend.daemon.data.server.DbfsServerBackend$$anonfun$receive$3.applyOrElse(DbfsServerBackend.scala:288)\n\tat com.databricks.backend.daemon.data.server.DbfsServerBackend$$anonfun$receive$3.applyOrElse(DbfsServerBackend.scala:248)\n\tat com.databricks.rpc.ServerBackend.$anonfun$internalReceive$2(ServerBackend.scala:53)\n\tat com.databricks.rpc.ServerBackend$$anonfun$commonReceive$1.applyOrElse(ServerBackend.scala:80)\n\tat com.databricks.rpc.ServerBackend$$anonfun$commonReceive$1.applyOrElse(ServerBackend.scala:80)\n\tat com.databricks.rpc.ServerBackend.$anonfun$internalReceive$1(ServerBackend.scala:49)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$4(UsageLogging.scala:432)\n\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:240)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:235)\n\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:232)\n\tat com.databricks.rpc.ServerBackend.withAttributionContext(ServerBackend.scala:16)\n\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:277)\n\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:270)\n\tat com.databricks.rpc.ServerBackend.withAttributionTags(ServerBackend.scala:16)\n\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:413)\n\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:339)\n\tat com.databricks.rpc.ServerBackend.recordOperation(ServerBackend.scala:16)\n\tat com.databricks.rpc.ServerBackend.internalReceive(ServerBackend.scala:48)\n\tat com.databricks.rpc.JettyServer$RequestManager.$anonfun$handleRPC$2(JettyServer.scala:648)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat com.databricks.rpc.JettyServer$RequestManager.handleRPC(JettyServer.scala:648)\n\tat com.databricks.rpc.JettyServer$RequestManager.handleRequestAndRespond(JettyServer.scala:570)\n\tat com.databricks.rpc.JettyServer$RequestManager.$anonfun$handleHttp$4(JettyServer.scala:337)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:240)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:235)\n\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:232)\n\tat com.databricks.rpc.JettyServer$.withAttributionContext(JettyServer.scala:161)\n\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:277)\n\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:270)\n\tat com.databricks.rpc.JettyServer$.withAttributionTags(JettyServer.scala:161)\n\tat com.databricks.rpc.JettyServer$RequestManager.handleHttp(JettyServer.scala:325)\n\tat com.databricks.rpc.JettyServer$RequestManager.doPost(JettyServer.scala:264)\n\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:707)\n\tat com.databricks.rpc.HttpServletWithPatch.service(HttpServletWithPatch.scala:33)\n\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:790)\n\tat org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:848)\n\tat org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:585)\n\tat org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:515)\n\tat org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)\n\tat org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:134)\n\tat org.eclipse.jetty.server.Server.handle(Server.java:539)\n\tat org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:333)\n\tat org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:251)\n\tat org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:283)\n\tat org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:108)\n\tat org.eclipse.jetty.io.SelectChannelEndPoint$2.run(SelectChannelEndPoint.java:93)\n\tat org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.executeProduceConsume(ExecuteProduceConsume.java:303)\n\tat org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.produceConsume(ExecuteProduceConsume.java:148)\n\tat org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.run(ExecuteProduceConsume.java:136)\n\tat org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:671)\n\tat org.eclipse.jetty.util.thread.QueuedThreadPool$2.run(QueuedThreadPool.java:589)\n\t... 1 more\n</div>","errorSummary":"java.rmi.RemoteException: java.lang.IllegalArgumentException: requirement failed: Directory already mounted: /mnt/sparklogfile; nested exception is: ","arguments":{}}},"data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">ExecutionError</span>                            Traceback (most recent call last)\n<span class=\"ansi-green-fg\">&lt;command-3348328071542889&gt;</span> in <span class=\"ansi-cyan-fg\">&lt;module&gt;</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      2</span>    source <span class=\"ansi-blue-fg\">=</span> <span class=\"ansi-blue-fg\">&#34;wasbs://{0}@{1}.blob.core.windows.net&#34;</span><span class=\"ansi-blue-fg\">.</span>format<span class=\"ansi-blue-fg\">(</span>container<span class=\"ansi-blue-fg\">,</span> storage_account_name<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">,</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      3</span>    mount_point <span class=\"ansi-blue-fg\">=</span> <span class=\"ansi-blue-fg\">&#34;/mnt/sparklogfile&#34;</span><span class=\"ansi-blue-fg\">,</span>\n<span class=\"ansi-green-fg\">----&gt; 4</span><span class=\"ansi-red-fg\">    </span>extra_configs <span class=\"ansi-blue-fg\">=</span> <span class=\"ansi-blue-fg\">{</span><span class=\"ansi-blue-fg\">&#34;fs.azure.account.key.{0}.blob.core.windows.net&#34;</span><span class=\"ansi-blue-fg\">.</span>format<span class=\"ansi-blue-fg\">(</span>storage_account_name<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span> storage_account_key<span class=\"ansi-blue-fg\">}</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      5</span>   )\n\n<span class=\"ansi-green-fg\">/local_disk0/tmp/1614143901425-0/dbutils.py</span> in <span class=\"ansi-cyan-fg\">f_with_exception_handling</span><span class=\"ansi-blue-fg\">(*args, **kwargs)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    322</span>                     exc<span class=\"ansi-blue-fg\">.</span>__context__ <span class=\"ansi-blue-fg\">=</span> <span class=\"ansi-green-fg\">None</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    323</span>                     exc<span class=\"ansi-blue-fg\">.</span>__cause__ <span class=\"ansi-blue-fg\">=</span> <span class=\"ansi-green-fg\">None</span>\n<span class=\"ansi-green-fg\">--&gt; 324</span><span class=\"ansi-red-fg\">                     </span><span class=\"ansi-green-fg\">raise</span> exc\n<span class=\"ansi-green-intense-fg ansi-bold\">    325</span>             <span class=\"ansi-green-fg\">return</span> f_with_exception_handling\n<span class=\"ansi-green-intense-fg ansi-bold\">    326</span> \n\n<span class=\"ansi-red-fg\">ExecutionError</span>: An error occurred while calling o257.mount.\n: java.rmi.RemoteException: java.lang.IllegalArgumentException: requirement failed: Directory already mounted: /mnt/sparklogfile; nested exception is: \n\tjava.lang.IllegalArgumentException: requirement failed: Directory already mounted: /mnt/sparklogfile\n\tat com.databricks.backend.daemon.data.client.DbfsClient.send0(DbfsClient.scala:128)\n\tat com.databricks.backend.daemon.data.client.DbfsClient.sendIdempotent(DbfsClient.scala:68)\n\tat com.databricks.backend.daemon.dbutils.DBUtilsCore.mount(DBUtilsCore.scala:470)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n\tat py4j.Gateway.invoke(Gateway.java:295)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:251)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.lang.IllegalArgumentException: requirement failed: Directory already mounted: /mnt/sparklogfile\n\tat scala.Predef$.require(Predef.scala:281)\n\tat com.databricks.backend.daemon.data.server.DefaultMetadataManager.$anonfun$insertMount$1(MetadataManager.scala:404)\n\tat com.databricks.backend.daemon.data.server.DefaultMetadataManager.$anonfun$modifyAndVerify$1(MetadataManager.scala:670)\n\tat com.databricks.backend.daemon.data.server.DefaultMetadataManager.withRetries(MetadataManager.scala:504)\n\tat com.databricks.backend.daemon.data.server.DefaultMetadataManager.modifyAndVerify(MetadataManager.scala:659)\n\tat com.databricks.backend.daemon.data.server.DefaultMetadataManager.insertMount(MetadataManager.scala:412)\n\tat com.databricks.backend.daemon.data.server.handler.MountHandler.receive(MountHandler.scala:89)\n\tat com.databricks.backend.daemon.data.server.handler.CEMountHandler.receive(MountHandler.scala:141)\n\tat com.databricks.backend.daemon.data.server.session.SessionContext.$anonfun$queryHandlers$1(SessionContext.scala:97)\n\tat com.databricks.backend.daemon.data.server.session.SessionContext.$anonfun$queryHandlers$1$adapted(SessionContext.scala:96)\n\tat scala.collection.immutable.List.foreach(List.scala:392)\n\tat com.databricks.backend.daemon.data.server.session.SessionContext.queryHandlers(SessionContext.scala:96)\n\tat com.databricks.backend.daemon.data.server.DbfsServerBackend$$anonfun$receive$3.applyOrElse(DbfsServerBackend.scala:288)\n\tat com.databricks.backend.daemon.data.server.DbfsServerBackend$$anonfun$receive$3.applyOrElse(DbfsServerBackend.scala:248)\n\tat com.databricks.rpc.ServerBackend.$anonfun$internalReceive$2(ServerBackend.scala:53)\n\tat com.databricks.rpc.ServerBackend$$anonfun$commonReceive$1.applyOrElse(ServerBackend.scala:80)\n\tat com.databricks.rpc.ServerBackend$$anonfun$commonReceive$1.applyOrElse(ServerBackend.scala:80)\n\tat com.databricks.rpc.ServerBackend.$anonfun$internalReceive$1(ServerBackend.scala:49)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$4(UsageLogging.scala:432)\n\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:240)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:235)\n\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:232)\n\tat com.databricks.rpc.ServerBackend.withAttributionContext(ServerBackend.scala:16)\n\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:277)\n\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:270)\n\tat com.databricks.rpc.ServerBackend.withAttributionTags(ServerBackend.scala:16)\n\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:413)\n\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:339)\n\tat com.databricks.rpc.ServerBackend.recordOperation(ServerBackend.scala:16)\n\tat com.databricks.rpc.ServerBackend.internalReceive(ServerBackend.scala:48)\n\tat com.databricks.rpc.JettyServer$RequestManager.$anonfun$handleRPC$2(JettyServer.scala:648)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat com.databricks.rpc.JettyServer$RequestManager.handleRPC(JettyServer.scala:648)\n\tat com.databricks.rpc.JettyServer$RequestManager.handleRequestAndRespond(JettyServer.scala:570)\n\tat com.databricks.rpc.JettyServer$RequestManager.$anonfun$handleHttp$4(JettyServer.scala:337)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:240)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:235)\n\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:232)\n\tat com.databricks.rpc.JettyServer$.withAttributionContext(JettyServer.scala:161)\n\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:277)\n\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:270)\n\tat com.databricks.rpc.JettyServer$.withAttributionTags(JettyServer.scala:161)\n\tat com.databricks.rpc.JettyServer$RequestManager.handleHttp(JettyServer.scala:325)\n\tat com.databricks.rpc.JettyServer$RequestManager.doPost(JettyServer.scala:264)\n\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:707)\n\tat com.databricks.rpc.HttpServletWithPatch.service(HttpServletWithPatch.scala:33)\n\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:790)\n\tat org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:848)\n\tat org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:585)\n\tat org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:515)\n\tat org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)\n\tat org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:134)\n\tat org.eclipse.jetty.server.Server.handle(Server.java:539)\n\tat org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:333)\n\tat org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:251)\n\tat org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:283)\n\tat org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:108)\n\tat org.eclipse.jetty.io.SelectChannelEndPoint$2.run(SelectChannelEndPoint.java:93)\n\tat org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.executeProduceConsume(ExecuteProduceConsume.java:303)\n\tat org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.produceConsume(ExecuteProduceConsume.java:148)\n\tat org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.run(ExecuteProduceConsume.java:136)\n\tat org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:671)\n\tat org.eclipse.jetty.util.thread.QueuedThreadPool$2.run(QueuedThreadPool.java:589)\n\t... 1 more\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["# Create a view or table\n","\n","temp_table_name = \"CpuLogData\"\n","\n","df.createOrReplaceTempView(temp_table_name)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"bd82bb99-1479-4d5c-be10-8c36df0f1d44"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["highest_average_active_user_df = sqlContext.sql(\"select user_name,from_unixtime(ROUND(((count(*)*5)*60)/6,2),'HH:mm') as average_active_time_in_hours from `CpuLogData` where ( keyboard != 0 or mouse != 0 ) group by user_name order by average_active_time_in_hours desc\")\n","display(highest_average_active_user_df)\n","highest_average_active_user_df.write.option(\"header\",True) \\\n"," .csv(\"/mnt/sparklogfile/highest_average_active_user_df.csv\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"93b619a9-68ee-426a-b4d3-3144b0c86151"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[["deepshukla292@gmail.com","06:35"],["iamnzm@outlook.com","06:22"],["sharlawar77@gmail.com","06:20"],["salinabodale73@gmail.com","06:06"],["rahilstar11@gmail.com","05:32"],["markfernandes66@gmail.com","05:24"],["bhagyashrichalke21@gmail.com","05:00"],["damodharn21@gmail.com","02:39"]],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"user_name","type":"\"string\"","metadata":"{}"},{"name":"average_active_time_in_hours","type":"\"string\"","metadata":"{}"}],"aggError":"","aggData":[],"addedWidgets":{},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>user_name</th><th>average_active_time_in_hours</th></tr></thead><tbody><tr><td>deepshukla292@gmail.com</td><td>06:35</td></tr><tr><td>iamnzm@outlook.com</td><td>06:22</td></tr><tr><td>sharlawar77@gmail.com</td><td>06:20</td></tr><tr><td>salinabodale73@gmail.com</td><td>06:06</td></tr><tr><td>rahilstar11@gmail.com</td><td>05:32</td></tr><tr><td>markfernandes66@gmail.com</td><td>05:24</td></tr><tr><td>bhagyashrichalke21@gmail.com</td><td>05:00</td></tr><tr><td>damodharn21@gmail.com</td><td>02:39</td></tr></tbody></table></div>"]}}],"execution_count":0},{"cell_type":"code","source":["lowest_average_active_user_df = sqlContext.sql(\"select user_name,from_unixtime(ROUND(((count(*)*5)*60)/6,2),'HH:mm')  as average_active_time_in_hours from `CpuLogData` where (keyboard != 0 or mouse != 0 ) group by user_name order by average_active_time_in_hours limit 1\")\n","display(lowest_average_active_user_df)\n","lowest_average_active_user_df.write.option(\"header\",True) \\\n"," .csv(\"/mnt/sparklogfile/lowest_average_active_user_df.csv\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"93cf43a9-c70d-464c-8e22-73a81ca569fa"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[["damodharn21@gmail.com","02:39"]],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"user_name","type":"\"string\"","metadata":"{}"},{"name":"average_active_time_in_hours","type":"\"string\"","metadata":"{}"}],"aggError":"","aggData":[],"addedWidgets":{},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>user_name</th><th>average_active_time_in_hours</th></tr></thead><tbody><tr><td>damodharn21@gmail.com</td><td>02:39</td></tr></tbody></table></div>"]}}],"execution_count":0},{"cell_type":"code","source":["# Highest Average Idle hour \n","highest_average_idle_user_df = sqlContext.sql(\"select user_name,from_unixtime(ROUND(((count(*)*5)*60)/6,2),'HH:mm') as average_idle_time_in_hours from `CpuLogData` where (keyboard == 0 and mouse == 0 ) group by user_name order by average_idle_time_in_hours desc limit 1 \")\n","display(highest_average_idle_user_df)\n","highest_average_idle_user_df.write.option(\"header\",True) \\\n"," .csv(\"/mnt/sparklogfile/highest_average_idle_user_df.csv\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"db9631f6-bb4a-42ca-8a3c-0d48af932331"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[["iamnzm@outlook.com","02:09"]],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"user_name","type":"\"string\"","metadata":"{}"},{"name":"average_idle_time_in_hours","type":"\"string\"","metadata":"{}"}],"aggError":"","aggData":[],"addedWidgets":{},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>user_name</th><th>average_idle_time_in_hours</th></tr></thead><tbody><tr><td>iamnzm@outlook.com</td><td>02:09</td></tr></tbody></table></div>"]}}],"execution_count":0},{"cell_type":"code","source":["\n","daily_session_start_time_df = sqlContext.sql(\"select Date,to_timestamp(min(DateTime)) as DateTime from CpuLogData group by Date order by Date\")\n","temp_table_name = \"view_session_start_time\"\n","daily_session_start_time_df.createOrReplaceTempView(temp_table_name)\n","display(daily_session_start_time_df)\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"20d453ce-9426-4ae8-a996-1e3ac0ef01fd"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[["2019-09-16","2019-09-16T12:55:01.000+0000"],["2019-09-17","2019-09-17T08:25:01.000+0000"],["2019-09-18","2019-09-18T08:30:01.000+0000"],["2019-09-19","2019-09-19T08:40:02.000+0000"],["2019-09-20","2019-09-20T09:05:01.000+0000"],["2019-09-21","2019-09-21T09:10:01.000+0000"]],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"Date","type":"\"string\"","metadata":"{}"},{"name":"DateTime","type":"\"timestamp\"","metadata":"{}"}],"aggError":"","aggData":[],"addedWidgets":{},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>Date</th><th>DateTime</th></tr></thead><tbody><tr><td>2019-09-16</td><td>2019-09-16T12:55:01.000+0000</td></tr><tr><td>2019-09-17</td><td>2019-09-17T08:25:01.000+0000</td></tr><tr><td>2019-09-18</td><td>2019-09-18T08:30:01.000+0000</td></tr><tr><td>2019-09-19</td><td>2019-09-19T08:40:02.000+0000</td></tr><tr><td>2019-09-20</td><td>2019-09-20T09:05:01.000+0000</td></tr><tr><td>2019-09-21</td><td>2019-09-21T09:10:01.000+0000</td></tr></tbody></table></div>"]}}],"execution_count":0},{"cell_type":"code","source":["daily_login_time_df = sqlContext.sql(\"select user_name,Date,min(DateTime) as Datetime from `CpuLogData` where (keyboard != 0 or mouse != 0) group by user_name,Date order by Date\")\n","temp_table_name = \"view_daily_login_time\"\n","daily_login_time_df.createOrReplaceTempView(temp_table_name)\n","display(daily_login_time_df)\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"75735561-591b-4d7c-9640-41700add9767"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[["deepshukla292@gmail.com","2019-09-16","2019-09-16T13:00:01.000+0000"],["salinabodale73@gmail.com","2019-09-16","2019-09-16T12:55:02.000+0000"],["rahilstar11@gmail.com","2019-09-16","2019-09-16T13:00:03.000+0000"],["iamnzm@outlook.com","2019-09-16","2019-09-16T13:00:01.000+0000"],["bhagyashrichalke21@gmail.com","2019-09-16","2019-09-16T12:55:01.000+0000"],["sharlawar77@gmail.com","2019-09-16","2019-09-16T13:00:04.000+0000"],["markfernandes66@gmail.com","2019-09-17","2019-09-17T10:50:01.000+0000"],["deepshukla292@gmail.com","2019-09-17","2019-09-17T09:30:01.000+0000"],["sharlawar77@gmail.com","2019-09-17","2019-09-17T10:45:02.000+0000"],["salinabodale73@gmail.com","2019-09-17","2019-09-17T10:15:01.000+0000"],["rahilstar11@gmail.com","2019-09-17","2019-09-17T09:40:01.000+0000"],["iamnzm@outlook.com","2019-09-17","2019-09-17T08:35:01.000+0000"],["bhagyashrichalke21@gmail.com","2019-09-17","2019-09-17T10:10:02.000+0000"],["bhagyashrichalke21@gmail.com","2019-09-18","2019-09-18T10:15:01.000+0000"],["sharlawar77@gmail.com","2019-09-18","2019-09-18T09:05:01.000+0000"],["markfernandes66@gmail.com","2019-09-18","2019-09-18T09:00:01.000+0000"],["iamnzm@outlook.com","2019-09-18","2019-09-18T08:30:01.000+0000"],["rahilstar11@gmail.com","2019-09-18","2019-09-18T10:15:02.000+0000"],["salinabodale73@gmail.com","2019-09-18","2019-09-18T10:10:01.000+0000"],["deepshukla292@gmail.com","2019-09-18","2019-09-18T09:05:01.000+0000"],["bhagyashrichalke21@gmail.com","2019-09-19","2019-09-19T10:20:01.000+0000"],["salinabodale73@gmail.com","2019-09-19","2019-09-19T10:20:01.000+0000"],["markfernandes66@gmail.com","2019-09-19","2019-09-19T09:10:01.000+0000"],["deepshukla292@gmail.com","2019-09-19","2019-09-19T09:05:01.000+0000"],["damodharn21@gmail.com","2019-09-19","2019-09-19T10:35:03.000+0000"],["sharlawar77@gmail.com","2019-09-19","2019-09-19T10:10:01.000+0000"],["rahilstar11@gmail.com","2019-09-19","2019-09-19T10:30:08.000+0000"],["iamnzm@outlook.com","2019-09-19","2019-09-19T08:40:02.000+0000"],["iamnzm@outlook.com","2019-09-20","2019-09-20T10:00:01.000+0000"],["damodharn21@gmail.com","2019-09-20","2019-09-20T10:35:02.000+0000"],["sharlawar77@gmail.com","2019-09-20","2019-09-20T09:05:01.000+0000"],["rahilstar11@gmail.com","2019-09-20","2019-09-20T10:20:02.000+0000"],["markfernandes66@gmail.com","2019-09-20","2019-09-20T10:25:01.000+0000"],["salinabodale73@gmail.com","2019-09-20","2019-09-20T10:25:01.000+0000"],["bhagyashrichalke21@gmail.com","2019-09-20","2019-09-20T10:25:01.000+0000"],["salinabodale73@gmail.com","2019-09-21","2019-09-21T11:20:01.000+0000"],["markfernandes66@gmail.com","2019-09-21","2019-09-21T11:20:01.000+0000"],["deepshukla292@gmail.com","2019-09-21","2019-09-21T09:10:01.000+0000"],["bhagyashrichalke21@gmail.com","2019-09-21","2019-09-21T11:20:02.000+0000"],["sharlawar77@gmail.com","2019-09-21","2019-09-21T11:25:01.000+0000"],["damodharn21@gmail.com","2019-09-21","2019-09-21T11:40:03.000+0000"],["iamnzm@outlook.com","2019-09-21","2019-09-21T11:25:01.000+0000"],["rahilstar11@gmail.com","2019-09-21","2019-09-21T11:20:01.000+0000"]],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"user_name","type":"\"string\"","metadata":"{}"},{"name":"Date","type":"\"string\"","metadata":"{}"},{"name":"Datetime","type":"\"timestamp\"","metadata":"{}"}],"aggError":"","aggData":[],"addedWidgets":{},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>user_name</th><th>Date</th><th>Datetime</th></tr></thead><tbody><tr><td>deepshukla292@gmail.com</td><td>2019-09-16</td><td>2019-09-16T13:00:01.000+0000</td></tr><tr><td>salinabodale73@gmail.com</td><td>2019-09-16</td><td>2019-09-16T12:55:02.000+0000</td></tr><tr><td>rahilstar11@gmail.com</td><td>2019-09-16</td><td>2019-09-16T13:00:03.000+0000</td></tr><tr><td>iamnzm@outlook.com</td><td>2019-09-16</td><td>2019-09-16T13:00:01.000+0000</td></tr><tr><td>bhagyashrichalke21@gmail.com</td><td>2019-09-16</td><td>2019-09-16T12:55:01.000+0000</td></tr><tr><td>sharlawar77@gmail.com</td><td>2019-09-16</td><td>2019-09-16T13:00:04.000+0000</td></tr><tr><td>markfernandes66@gmail.com</td><td>2019-09-17</td><td>2019-09-17T10:50:01.000+0000</td></tr><tr><td>deepshukla292@gmail.com</td><td>2019-09-17</td><td>2019-09-17T09:30:01.000+0000</td></tr><tr><td>sharlawar77@gmail.com</td><td>2019-09-17</td><td>2019-09-17T10:45:02.000+0000</td></tr><tr><td>salinabodale73@gmail.com</td><td>2019-09-17</td><td>2019-09-17T10:15:01.000+0000</td></tr><tr><td>rahilstar11@gmail.com</td><td>2019-09-17</td><td>2019-09-17T09:40:01.000+0000</td></tr><tr><td>iamnzm@outlook.com</td><td>2019-09-17</td><td>2019-09-17T08:35:01.000+0000</td></tr><tr><td>bhagyashrichalke21@gmail.com</td><td>2019-09-17</td><td>2019-09-17T10:10:02.000+0000</td></tr><tr><td>bhagyashrichalke21@gmail.com</td><td>2019-09-18</td><td>2019-09-18T10:15:01.000+0000</td></tr><tr><td>sharlawar77@gmail.com</td><td>2019-09-18</td><td>2019-09-18T09:05:01.000+0000</td></tr><tr><td>markfernandes66@gmail.com</td><td>2019-09-18</td><td>2019-09-18T09:00:01.000+0000</td></tr><tr><td>iamnzm@outlook.com</td><td>2019-09-18</td><td>2019-09-18T08:30:01.000+0000</td></tr><tr><td>rahilstar11@gmail.com</td><td>2019-09-18</td><td>2019-09-18T10:15:02.000+0000</td></tr><tr><td>salinabodale73@gmail.com</td><td>2019-09-18</td><td>2019-09-18T10:10:01.000+0000</td></tr><tr><td>deepshukla292@gmail.com</td><td>2019-09-18</td><td>2019-09-18T09:05:01.000+0000</td></tr><tr><td>bhagyashrichalke21@gmail.com</td><td>2019-09-19</td><td>2019-09-19T10:20:01.000+0000</td></tr><tr><td>salinabodale73@gmail.com</td><td>2019-09-19</td><td>2019-09-19T10:20:01.000+0000</td></tr><tr><td>markfernandes66@gmail.com</td><td>2019-09-19</td><td>2019-09-19T09:10:01.000+0000</td></tr><tr><td>deepshukla292@gmail.com</td><td>2019-09-19</td><td>2019-09-19T09:05:01.000+0000</td></tr><tr><td>damodharn21@gmail.com</td><td>2019-09-19</td><td>2019-09-19T10:35:03.000+0000</td></tr><tr><td>sharlawar77@gmail.com</td><td>2019-09-19</td><td>2019-09-19T10:10:01.000+0000</td></tr><tr><td>rahilstar11@gmail.com</td><td>2019-09-19</td><td>2019-09-19T10:30:08.000+0000</td></tr><tr><td>iamnzm@outlook.com</td><td>2019-09-19</td><td>2019-09-19T08:40:02.000+0000</td></tr><tr><td>iamnzm@outlook.com</td><td>2019-09-20</td><td>2019-09-20T10:00:01.000+0000</td></tr><tr><td>damodharn21@gmail.com</td><td>2019-09-20</td><td>2019-09-20T10:35:02.000+0000</td></tr><tr><td>sharlawar77@gmail.com</td><td>2019-09-20</td><td>2019-09-20T09:05:01.000+0000</td></tr><tr><td>rahilstar11@gmail.com</td><td>2019-09-20</td><td>2019-09-20T10:20:02.000+0000</td></tr><tr><td>markfernandes66@gmail.com</td><td>2019-09-20</td><td>2019-09-20T10:25:01.000+0000</td></tr><tr><td>salinabodale73@gmail.com</td><td>2019-09-20</td><td>2019-09-20T10:25:01.000+0000</td></tr><tr><td>bhagyashrichalke21@gmail.com</td><td>2019-09-20</td><td>2019-09-20T10:25:01.000+0000</td></tr><tr><td>salinabodale73@gmail.com</td><td>2019-09-21</td><td>2019-09-21T11:20:01.000+0000</td></tr><tr><td>markfernandes66@gmail.com</td><td>2019-09-21</td><td>2019-09-21T11:20:01.000+0000</td></tr><tr><td>deepshukla292@gmail.com</td><td>2019-09-21</td><td>2019-09-21T09:10:01.000+0000</td></tr><tr><td>bhagyashrichalke21@gmail.com</td><td>2019-09-21</td><td>2019-09-21T11:20:02.000+0000</td></tr><tr><td>sharlawar77@gmail.com</td><td>2019-09-21</td><td>2019-09-21T11:25:01.000+0000</td></tr><tr><td>damodharn21@gmail.com</td><td>2019-09-21</td><td>2019-09-21T11:40:03.000+0000</td></tr><tr><td>iamnzm@outlook.com</td><td>2019-09-21</td><td>2019-09-21T11:25:01.000+0000</td></tr><tr><td>rahilstar11@gmail.com</td><td>2019-09-21</td><td>2019-09-21T11:20:01.000+0000</td></tr></tbody></table></div>"]}}],"execution_count":0},{"cell_type":"code","source":["late_users_df = sqlContext.sql(\"select lut.user_name,\\\n","Concat(cast(Round((Sum(lut.TimeDifference)/60)/6,2) AS INT),' Hour , ',Floor(Round((((Sum(lut.TimeDifference)/60)/6)%1)*60)),' Min') as average_late_time_in_hours, count(*) as daily_late_count from (Select lt.user_name,(unix_timestamp(lt.DateTime)-unix_timestamp(st.DateTime))/(60) as TimeDifference from view_daily_login_time as lt,view_session_start_time as st where lt.Date == st.Date) as lut where lut.TimeDifference != 0 group by lut.user_name order by average_late_time_in_hours desc\")\n","display(late_users_df)\n","late_users_df.write.option(\"header\",True) \\\n"," .csv(\"/mnt/sparklogfile/late_users_df.csv\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2ba170a3-f269-4f91-b6da-403d788636e7"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[["markfernandes66@gmail.com","1 Hour , 9 Min",5],["sharlawar77@gmail.com","1 Hour , 8 Min",5],["salinabodale73@gmail.com","1 Hour , 27 Min",6],["bhagyashrichalke21@gmail.com","1 Hour , 27 Min",5],["rahilstar11@gmail.com","1 Hour , 23 Min",6],["damodharn21@gmail.com","0 Hour , 59 Min",3],["iamnzm@outlook.com","0 Hour , 34 Min",4],["deepshukla292@gmail.com","0 Hour , 22 Min",4]],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"user_name","type":"\"string\"","metadata":"{}"},{"name":"average_late_time_in_hours","type":"\"string\"","metadata":"{}"},{"name":"daily_late_count","type":"\"long\"","metadata":"{}"}],"aggError":"","aggData":[],"addedWidgets":{},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>user_name</th><th>average_late_time_in_hours</th><th>daily_late_count</th></tr></thead><tbody><tr><td>markfernandes66@gmail.com</td><td>1 Hour , 9 Min</td><td>5</td></tr><tr><td>sharlawar77@gmail.com</td><td>1 Hour , 8 Min</td><td>5</td></tr><tr><td>salinabodale73@gmail.com</td><td>1 Hour , 27 Min</td><td>6</td></tr><tr><td>bhagyashrichalke21@gmail.com</td><td>1 Hour , 27 Min</td><td>5</td></tr><tr><td>rahilstar11@gmail.com</td><td>1 Hour , 23 Min</td><td>6</td></tr><tr><td>damodharn21@gmail.com</td><td>0 Hour , 59 Min</td><td>3</td></tr><tr><td>iamnzm@outlook.com</td><td>0 Hour , 34 Min</td><td>4</td></tr><tr><td>deepshukla292@gmail.com</td><td>0 Hour , 22 Min</td><td>4</td></tr></tbody></table></div>"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"<div class=\"ansiout\"><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">AnalysisException</span>                         Traceback (most recent call last)\n<span class=\"ansi-green-fg\">&lt;command-3916484300341808&gt;</span> in <span class=\"ansi-cyan-fg\">&lt;module&gt;</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      3</span> display<span class=\"ansi-blue-fg\">(</span>late_users_df<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      4</span> late_users_df<span class=\"ansi-blue-fg\">.</span>write<span class=\"ansi-blue-fg\">.</span>option<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;header&#34;</span><span class=\"ansi-blue-fg\">,</span><span class=\"ansi-green-fg\">True</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-red-fg\"> </span><span class=\"ansi-red-fg\">\\</span>\n<span class=\"ansi-green-fg\">----&gt; 5</span><span class=\"ansi-red-fg\">  </span><span class=\"ansi-blue-fg\">.</span>csv<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;/mnt/sparklogfile/late_users_df.csv&#34;</span><span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/readwriter.py</span> in <span class=\"ansi-cyan-fg\">csv</span><span class=\"ansi-blue-fg\">(self, path, mode, compression, sep, quote, escape, header, nullValue, escapeQuotes, quoteAll, dateFormat, timestampFormat, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, charToEscapeQuoteEscaping, encoding, emptyValue, lineSep)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1028</span>                        charToEscapeQuoteEscaping<span class=\"ansi-blue-fg\">=</span>charToEscapeQuoteEscaping<span class=\"ansi-blue-fg\">,</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1029</span>                        encoding=encoding, emptyValue=emptyValue, lineSep=lineSep)\n<span class=\"ansi-green-fg\">-&gt; 1030</span><span class=\"ansi-red-fg\">         </span>self<span class=\"ansi-blue-fg\">.</span>_jwrite<span class=\"ansi-blue-fg\">.</span>csv<span class=\"ansi-blue-fg\">(</span>path<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1031</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">   1032</span>     <span class=\"ansi-blue-fg\">@</span>since<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-cyan-fg\">1.5</span><span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py</span> in <span class=\"ansi-cyan-fg\">__call__</span><span class=\"ansi-blue-fg\">(self, *args)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1303</span>         answer <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>gateway_client<span class=\"ansi-blue-fg\">.</span>send_command<span class=\"ansi-blue-fg\">(</span>command<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1304</span>         return_value = get_return_value(\n<span class=\"ansi-green-fg\">-&gt; 1305</span><span class=\"ansi-red-fg\">             answer, self.gateway_client, self.target_id, self.name)\n</span><span class=\"ansi-green-intense-fg ansi-bold\">   1306</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">   1307</span>         <span class=\"ansi-green-fg\">for</span> temp_arg <span class=\"ansi-green-fg\">in</span> temp_args<span class=\"ansi-blue-fg\">:</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/utils.py</span> in <span class=\"ansi-cyan-fg\">deco</span><span class=\"ansi-blue-fg\">(*a, **kw)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    131</span>                 <span class=\"ansi-red-fg\"># Hide where the exception came from that shows a non-Pythonic</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    132</span>                 <span class=\"ansi-red-fg\"># JVM exception message.</span>\n<span class=\"ansi-green-fg\">--&gt; 133</span><span class=\"ansi-red-fg\">                 </span>raise_from<span class=\"ansi-blue-fg\">(</span>converted<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    134</span>             <span class=\"ansi-green-fg\">else</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    135</span>                 <span class=\"ansi-green-fg\">raise</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/utils.py</span> in <span class=\"ansi-cyan-fg\">raise_from</span><span class=\"ansi-blue-fg\">(e)</span>\n\n<span class=\"ansi-red-fg\">AnalysisException</span>: path dbfs:/mnt/sparklogfile/late_users_df.csv already exists.;</div>","errorSummary":"<span class=\"ansi-red-fg\">AnalysisException</span>: path dbfs:/mnt/sparklogfile/late_users_df.csv already exists.;","arguments":{}}},"data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">AnalysisException</span>                         Traceback (most recent call last)\n<span class=\"ansi-green-fg\">&lt;command-3916484300341808&gt;</span> in <span class=\"ansi-cyan-fg\">&lt;module&gt;</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      3</span> display<span class=\"ansi-blue-fg\">(</span>late_users_df<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      4</span> late_users_df<span class=\"ansi-blue-fg\">.</span>write<span class=\"ansi-blue-fg\">.</span>option<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;header&#34;</span><span class=\"ansi-blue-fg\">,</span><span class=\"ansi-green-fg\">True</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-red-fg\"> </span><span class=\"ansi-red-fg\">\\</span>\n<span class=\"ansi-green-fg\">----&gt; 5</span><span class=\"ansi-red-fg\">  </span><span class=\"ansi-blue-fg\">.</span>csv<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;/mnt/sparklogfile/late_users_df.csv&#34;</span><span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/readwriter.py</span> in <span class=\"ansi-cyan-fg\">csv</span><span class=\"ansi-blue-fg\">(self, path, mode, compression, sep, quote, escape, header, nullValue, escapeQuotes, quoteAll, dateFormat, timestampFormat, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, charToEscapeQuoteEscaping, encoding, emptyValue, lineSep)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1028</span>                        charToEscapeQuoteEscaping<span class=\"ansi-blue-fg\">=</span>charToEscapeQuoteEscaping<span class=\"ansi-blue-fg\">,</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1029</span>                        encoding=encoding, emptyValue=emptyValue, lineSep=lineSep)\n<span class=\"ansi-green-fg\">-&gt; 1030</span><span class=\"ansi-red-fg\">         </span>self<span class=\"ansi-blue-fg\">.</span>_jwrite<span class=\"ansi-blue-fg\">.</span>csv<span class=\"ansi-blue-fg\">(</span>path<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1031</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">   1032</span>     <span class=\"ansi-blue-fg\">@</span>since<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-cyan-fg\">1.5</span><span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py</span> in <span class=\"ansi-cyan-fg\">__call__</span><span class=\"ansi-blue-fg\">(self, *args)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1303</span>         answer <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>gateway_client<span class=\"ansi-blue-fg\">.</span>send_command<span class=\"ansi-blue-fg\">(</span>command<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1304</span>         return_value = get_return_value(\n<span class=\"ansi-green-fg\">-&gt; 1305</span><span class=\"ansi-red-fg\">             answer, self.gateway_client, self.target_id, self.name)\n</span><span class=\"ansi-green-intense-fg ansi-bold\">   1306</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">   1307</span>         <span class=\"ansi-green-fg\">for</span> temp_arg <span class=\"ansi-green-fg\">in</span> temp_args<span class=\"ansi-blue-fg\">:</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/utils.py</span> in <span class=\"ansi-cyan-fg\">deco</span><span class=\"ansi-blue-fg\">(*a, **kw)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    131</span>                 <span class=\"ansi-red-fg\"># Hide where the exception came from that shows a non-Pythonic</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    132</span>                 <span class=\"ansi-red-fg\"># JVM exception message.</span>\n<span class=\"ansi-green-fg\">--&gt; 133</span><span class=\"ansi-red-fg\">                 </span>raise_from<span class=\"ansi-blue-fg\">(</span>converted<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    134</span>             <span class=\"ansi-green-fg\">else</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    135</span>                 <span class=\"ansi-green-fg\">raise</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/utils.py</span> in <span class=\"ansi-cyan-fg\">raise_from</span><span class=\"ansi-blue-fg\">(e)</span>\n\n<span class=\"ansi-red-fg\">AnalysisException</span>: path dbfs:/mnt/sparklogfile/late_users_df.csv already exists.;</div>"]}}],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"2021-02-21 - DBFS Example (2)","dashboards":[],"language":"python","widgets":{},"notebookOrigID":2980103652348237}},"nbformat":4,"nbformat_minor":0}